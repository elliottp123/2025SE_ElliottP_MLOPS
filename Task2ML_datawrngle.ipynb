{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Wrangling for Student Performance Datasets\n",
        "\n",
        "This notebook demonstrates data wrangling for the student performance datasets (`mat.arff`, `por.arff`, and `dataset.csv`). The processed data will be saved in the `processed_data` folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dependancies and frameworks\n",
        "Load the two required dependencies:\n",
        "\n",
        "- [Pandas](https://pandas.pydata.org/) is library that allows us to handle data for wrangling and visualisation.\n",
        "- [sklearn](https://scikit-learn.org/stable/) A framework for training Machine Learning, we will use this for wrangling, but also applies to training and testing.\n",
        "- [os, IO](https://docs.python.org/3/library) Default packages installed with python, allows us to create, save and edit files with basic string functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import frameworks\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import os\n",
        "from io import StringIO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load ARFF files as CSV\n",
        "def load_arff_as_csv(filepath):\n",
        "    with open(filepath, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "    data_start = False\n",
        "    data = []\n",
        "    for line in lines:\n",
        "        if data_start:\n",
        "            data.append(line.strip())\n",
        "        if line.strip().lower() == '@data':\n",
        "            data_start = True\n",
        "    return pd.read_csv(StringIO('\\n'.join(data)), header=None)\n",
        "\n",
        "# Load ARFF files\n",
        "mat_df = load_arff_as_csv('data/mat.arff')\n",
        "por_df = load_arff_as_csv('data/por.arff')\n",
        "\n",
        "# Set column names for mat.arff\n",
        "mat_columns = ['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3']\n",
        "mat_df.columns = mat_columns\n",
        "\n",
        "# Set column names for por.arff\n",
        "por_columns = ['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3']\n",
        "por_df.columns = por_columns\n",
        "\n",
        "# Load CSV file (which is actually in ARFF format)\n",
        "csv_columns = ['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G3']\n",
        "\n",
        "# Use the same ARFF loading function for dataset.csv\n",
        "csv_df = load_arff_as_csv('data/dataset.csv')\n",
        "csv_df.columns = csv_columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dealing with null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove Null values\n",
        "def remove_nulls(df):\n",
        "    df = df.dropna()\n",
        "    return df\n",
        "\n",
        "mat_df = remove_nulls(mat_df)\n",
        "por_df = remove_nulls(por_df)\n",
        "csv_df = remove_nulls(csv_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Remove Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove duplicates\n",
        "def remove_duplicates(df):\n",
        "    df = df.drop_duplicates()\n",
        "    return df\n",
        "\n",
        "mat_df = remove_duplicates(mat_df)\n",
        "por_df = remove_duplicates(por_df)\n",
        "csv_df = remove_duplicates(csv_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Replace data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace data\n",
        "def replace_data(df, column):\n",
        "    df[column] = df[column].apply(lambda x: x.lower())\n",
        "    return df\n",
        "\n",
        "mat_df = replace_data(mat_df, 'sex')\n",
        "por_df = replace_data(por_df, 'sex')\n",
        "csv_df = replace_data(csv_df, 'sex')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Remove outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove outliers\n",
        "def remove_outliers(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    df = df[(df[column] >= Q1 - 1.5 * IQR) & (df[column] <= Q3 + 1.5 * IQR)]\n",
        "    return df\n",
        "\n",
        "mat_df = remove_outliers(mat_df, 'age')\n",
        "por_df = remove_outliers(por_df, 'age')\n",
        "csv_df = remove_outliers(csv_df, 'age')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Scaling features to a common range"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale features\n",
        "# this would normally be something like this\n",
        "# scaler = MinMaxScaler()\n",
        "# mat_df[['age', 'absences', 'G3']] = scaler.fit_transform(mat_df[['age', 'absences', 'G3']])\n",
        "# por_df[['age', 'absences', 'G3']] = scaler.fit_transform(por_df[['age', 'absences', 'G3']])\n",
        "# csv_df[['age', 'absences', 'G3']] = scaler.fit_transform(csv_df[['age', 'absences', 'G3']])\n",
        "\n",
        "# but for this use case, this data should not be scaled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Save the wrangled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create directory if it doesn't exist\n",
        "os.makedirs('processed_data', exist_ok=True)\n",
        "\n",
        "# Save the processed data files\n",
        "mat_df.to_csv('processed_data/Pmat.csv', index=False)\n",
        "por_df.to_csv('processed_data/Ppor.csv', index=False)\n",
        "csv_df.to_csv('processed_data/Pdataset.csv', index=False)\n",
        "# Raw processed data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Split the data into training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of rows in csv_df: 648\n",
            "Number of rows in X_train: 518\n",
            "Number of rows in X_test: 130\n"
          ]
        }
      ],
      "source": [
        "# Split the data\n",
        "X = csv_df.drop('G3', axis=1)  # Features\n",
        "y = csv_df['G3']  # Target variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Debugging statements\n",
        "print(f\"Number of rows in csv_df: {len(csv_df)}\")\n",
        "print(f\"Number of rows in X_train: {len(X_train)}\")\n",
        "print(f\"Number of rows in X_test: {len(X_test)}\")\n",
        "\n",
        "# Save all processed datasets\n",
        "mat_df.to_csv('processed_data/Pmat.csv', index=False)\n",
        "por_df.to_csv('processed_data/Ppor.csv', index=False)\n",
        "csv_df.to_csv('processed_data/Pdataset.csv', index=False)\n",
        "\n",
        "# Save the split data\n",
        "X_train.to_csv('processed_data/X_train.csv', index=False)\n",
        "X_test.to_csv('processed_data/X_test.csv', index=False)\n",
        "y_train.to_csv('processed_data/y_train.csv', index=False)\n",
        "y_test.to_csv('processed_data/y_test.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
