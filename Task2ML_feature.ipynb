{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering\n",
        "\n",
        "### Engineered Features and Justifications\n",
        "\n",
        "1. **Gvg** - G1 and G2 average grade (integer of (G1 + G2) / 2)\n",
        "   - Justification: Averaging the first and second period grades provides a more stable measure of a student's performance over time, reducing the impact of any single period's anomalies.\n",
        "\n",
        "2. **Avgalc** - Average Dalc and Walc (integer of (Dalc + Walc) / 2)\n",
        "   - Justification: Combining workday and weekend alcohol consumption into a single average value gives a more comprehensive view of a student's overall alcohol consumption habits.\n",
        "\n",
        "3. **Bum** - A weighted sum of failures, absences, Dalc, Walc, inverted studytime, and freetime to indicate a student's tendency to fail, skip school, drink alcohol, not study, and have free time.\n",
        "   - Justification for weights:\n",
        "     - Failures are given a higher weight (2) because past class failures are a strong indicator of academic struggles.\n",
        "     - Absences are weighted at 1.5 as frequent absences can significantly impact academic performance.\n",
        "     - Both Dalc and Walc are weighted at 1 as alcohol consumption can affect both health and academic performance.\n",
        "     - Studytime is inverted (5 - studytime) and weighted at 1 because less study time can lead to poorer academic outcomes.\n",
        "     - Freetime is weighted at 1 as more free time might indicate less focus on academics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import dependancies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datasets loaded successfully:\n",
            "Mat: 394 rows, 33 columns\n",
            "Sample data types:\n",
            "school     int64\n",
            "sex        int64\n",
            "age        int64\n",
            "address    int64\n",
            "famsize    int64\n",
            "dtype: object\n",
            "\n",
            "Por: 648 rows, 33 columns\n",
            "Sample data types:\n",
            "school     int64\n",
            "sex        int64\n",
            "age        int64\n",
            "address    int64\n",
            "famsize    int64\n",
            "dtype: object\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def load_datasets():\n",
        "    \"\"\"Load encoded datasets\"\"\"\n",
        "    base_path = 'processed_data/'\n",
        "    datasets = {}\n",
        "    \n",
        "    try:\n",
        "        # Load full encoded datasets\n",
        "        datasets['Mat'] = pd.read_csv(f'{base_path}Pmat_full.csv')\n",
        "        datasets['Por'] = pd.read_csv(f'{base_path}Ppor_full.csv')\n",
        "        \n",
        "        print(\"Datasets loaded successfully:\")\n",
        "        for name, df in datasets.items():\n",
        "            print(f\"{name}: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "            print(f\"Sample data types:\\n{df.dtypes.head()}\\n\")\n",
        "        \n",
        "        return datasets\n",
        "        \n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error loading datasets: {e}\")\n",
        "        return None\n",
        "\n",
        "# Load all datasets\n",
        "datasets = load_datasets()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Calculating new features from existing features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_features(df):\n",
        "    \"\"\"Add engineered features to dataframe\"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # Calculate grade average\n",
        "    df['Gvg'] = df[['G1', 'G2']].mean(axis=1)\n",
        "    \n",
        "    # Calculate alcohol average\n",
        "    df['Avgalc'] = df[['Dalc', 'Walc']].mean(axis=1)\n",
        "    \n",
        "    # Calculate risk factor (Bum)\n",
        "    df['Bum'] = (2.0 * df['failures'] + \n",
        "                 1.5 * df['absences'] + \n",
        "                 1.0 * df['Dalc'] + \n",
        "                 1.0 * df['Walc'] + \n",
        "                 1.0 * (5 - df['studytime']) + \n",
        "                 1.0 * df['freetime'])\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Apply features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing Mat dataset...\n",
            "\n",
            "Processing Por dataset...\n"
          ]
        }
      ],
      "source": [
        "def process_and_save_datasets(datasets):\n",
        "    \"\"\"Process and split datasets by gender\"\"\"\n",
        "    base_path = 'processed_data/'\n",
        "    processed_datasets = {}\n",
        "    \n",
        "    for name, df in datasets.items():\n",
        "        print(f\"\\nProcessing {name} dataset...\")\n",
        "        try:\n",
        "            # Add engineered features\n",
        "            processed_df = calculate_features(df)\n",
        "            \n",
        "            # Split by gender (using encoded values)\n",
        "            processed_df_fe = processed_df[processed_df['sex'] == 0].copy()\n",
        "            processed_df_m = processed_df[processed_df['sex'] == 1].copy()\n",
        "            \n",
        "            # Save enhanced gender-split datasets\n",
        "            processed_df_fe.to_csv(f'{base_path}X_{name}FE_enhanced.csv', index=False)\n",
        "            processed_df_m.to_csv(f'{base_path}X_{name}M_enhanced.csv', index=False)\n",
        "            \n",
        "            processed_datasets[f'{name}FE'] = processed_df_fe\n",
        "            processed_datasets[f'{name}M'] = processed_df_m\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {name} dataset: {e}\")\n",
        "            continue\n",
        "    \n",
        "    return processed_datasets\n",
        "\n",
        "# Process datasets\n",
        "processed_datasets = process_and_save_datasets(datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyse features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Feature Engineering Statistics:\n",
            "\n",
            "MatFE Dataset Statistics:\n",
            "\n",
            "Gvg:\n",
            "  Mean: 10.50\n",
            "  Std: 3.30\n",
            "  Min: 2.00\n",
            "  Max: 18.50\n",
            "\n",
            "Avgalc:\n",
            "  Mean: 1.61\n",
            "  Std: 0.73\n",
            "  Min: 1.00\n",
            "  Max: 5.00\n",
            "\n",
            "Bum:\n",
            "  Mean: 18.87\n",
            "  Std: 14.70\n",
            "  Min: 6.00\n",
            "  Max: 118.50\n",
            "\n",
            "MatM Dataset Statistics:\n",
            "\n",
            "Gvg:\n",
            "  Mean: 11.17\n",
            "  Std: 3.50\n",
            "  Min: 2.50\n",
            "  Max: 19.00\n",
            "\n",
            "Avgalc:\n",
            "  Mean: 2.18\n",
            "  Std: 1.13\n",
            "  Min: 1.00\n",
            "  Max: 5.00\n",
            "\n",
            "Bum:\n",
            "  Mean: 19.42\n",
            "  Std: 10.16\n",
            "  Min: 5.00\n",
            "  Max: 68.00\n",
            "\n",
            "PorFE Dataset Statistics:\n",
            "\n",
            "Gvg:\n",
            "  Mean: 11.73\n",
            "  Std: 2.78\n",
            "  Min: 2.50\n",
            "  Max: 18.50\n",
            "\n",
            "Avgalc:\n",
            "  Mean: 1.61\n",
            "  Std: 0.74\n",
            "  Min: 1.00\n",
            "  Max: 5.00\n",
            "\n",
            "Bum:\n",
            "  Mean: 14.94\n",
            "  Std: 7.69\n",
            "  Min: 4.00\n",
            "  Max: 59.00\n",
            "\n",
            "PorM Dataset Statistics:\n",
            "\n",
            "Gvg:\n",
            "  Mean: 11.15\n",
            "  Std: 2.63\n",
            "  Min: 2.00\n",
            "  Max: 18.00\n",
            "\n",
            "Avgalc:\n",
            "  Mean: 2.28\n",
            "  Std: 1.16\n",
            "  Min: 1.00\n",
            "  Max: 5.00\n",
            "\n",
            "Bum:\n",
            "  Mean: 17.35\n",
            "  Std: 8.41\n",
            "  Min: 6.00\n",
            "  Max: 53.00\n"
          ]
        }
      ],
      "source": [
        "def analyze_features(processed_datasets):\n",
        "    print(\"\\nFeature Engineering Statistics:\")\n",
        "    \n",
        "    for name, df in processed_datasets.items():\n",
        "        print(f\"\\n{name} Dataset Statistics:\")\n",
        "        \n",
        "        new_features = ['Gvg', 'Avgalc', 'Bum']\n",
        "        for feature in new_features:\n",
        "            if feature in df.columns:\n",
        "                print(f\"\\n{feature}:\")\n",
        "                print(f\"  Mean: {df[feature].mean():.2f}\")\n",
        "                print(f\"  Std: {df[feature].std():.2f}\")\n",
        "                print(f\"  Min: {df[feature].min():.2f}\")\n",
        "                print(f\"  Max: {df[feature].max():.2f}\")\n",
        "            else:\n",
        "                print(f\"\\n{feature}: Not available for this dataset\")\n",
        "\n",
        "# Analyze all processed datasets\n",
        "if 'processed_datasets' in locals():\n",
        "    analyze_features(processed_datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Splitting and saving this data for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing MatFE dataset for train-test split...\n",
            "\n",
            "MatFE:\n",
            "Training: 145 samples\n",
            "Testing: 63 samples\n",
            "\n",
            "Processing MatM dataset for train-test split...\n",
            "\n",
            "MatM:\n",
            "Training: 130 samples\n",
            "Testing: 56 samples\n",
            "\n",
            "Processing PorFE dataset for train-test split...\n",
            "\n",
            "PorFE:\n",
            "Training: 268 samples\n",
            "Testing: 115 samples\n",
            "\n",
            "Processing PorM dataset for train-test split...\n",
            "\n",
            "PorM:\n",
            "Training: 185 samples\n",
            "Testing: 80 samples\n"
          ]
        }
      ],
      "source": [
        "def split_save_and_print(data, name, test_size=0.3, random_state=42):\n",
        "    \"\"\"Split dataset into training and testing sets\"\"\"\n",
        "    if len(data) == 0:\n",
        "        print(f\"\\nWarning: {name} dataset is empty, skipping split\")\n",
        "        return None, None, None, None\n",
        "        \n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        data.drop(['G1', 'G2', 'G3'], axis=1),\n",
        "        data[['G1', 'G2', 'G3']],\n",
        "        test_size=test_size,\n",
        "        random_state=random_state\n",
        "    )\n",
        "    \n",
        "    # Save splits\n",
        "    base_path = f'processed_data/{name.lower()}'\n",
        "    X_train.to_csv(f'{base_path}_X_train.csv', index=False)\n",
        "    X_test.to_csv(f'{base_path}_X_test.csv', index=False)\n",
        "    y_train.to_csv(f'{base_path}_y_train.csv', index=False)\n",
        "    y_test.to_csv(f'{base_path}_y_test.csv', index=False)\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"Training: {X_train.shape[0]} samples\")\n",
        "    print(f\"Testing: {X_test.shape[0]} samples\")\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Process and save datasets\n",
        "if 'processed_datasets' in locals():\n",
        "    for name, df in processed_datasets.items():\n",
        "        print(f\"\\nProcessing {name} dataset for train-test split...\")\n",
        "        split_save_and_print(df, name)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
