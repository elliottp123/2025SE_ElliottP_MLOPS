{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering\n",
        "\n",
        "### Engineered Features and Justifications\n",
        "\n",
        "1. **Gvg** - G1 and G2 average grade (integer of (G1 + G2) / 2)\n",
        "   - Justification: Averaging the first and second period grades provides a more stable measure of a student's performance over time, reducing the impact of any single period's anomalies.\n",
        "\n",
        "2. **Avgalc** - Average Dalc and Walc (integer of (Dalc + Walc) / 2)\n",
        "   - Justification: Combining workday and weekend alcohol consumption into a single average value gives a more comprehensive view of a student's overall alcohol consumption habits.\n",
        "\n",
        "3. **Bum** - A weighted sum of failures, absences, Dalc, Walc, inverted studytime, and freetime to indicate a student's tendency to fail, skip school, drink alcohol, not study, and have free time.\n",
        "   - Justification for weights:\n",
        "     - Failures are given a higher weight (2) because past class failures are a strong indicator of academic struggles.\n",
        "     - Absences are weighted at 1.5 as frequent absences can significantly impact academic performance.\n",
        "     - Both Dalc and Walc are weighted at 1 as alcohol consumption can affect both health and academic performance.\n",
        "     - Studytime is inverted (5 - studytime) and weighted at 1 because less study time can lead to poorer academic outcomes.\n",
        "     - Freetime is weighted at 1 as more free time might indicate less focus on academics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import dependancies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Verifying datasets after loading:\n",
            "Pmat_full: 394 rows, ['school', 'sex', 'age', 'address', 'famsize'] columns\n",
            "Ppor_full: 648 rows, ['school', 'sex', 'age', 'address', 'famsize'] columns\n",
            "PmatM: 186 rows, ['school', 'sex', 'age', 'address', 'famsize'] columns\n",
            "PmatFE: 208 rows, ['school', 'sex', 'age', 'address', 'famsize'] columns\n",
            "PporM: 265 rows, ['school', 'sex', 'age', 'address', 'famsize'] columns\n",
            "PporFE: 383 rows, ['school', 'sex', 'age', 'address', 'famsize'] columns\n"
          ]
        }
      ],
      "source": [
        "# Load the datasets\n",
        "def load_processed_datasets():\n",
        "    \"\"\"Load all preprocessed datasets\"\"\"\n",
        "    base_path = 'processed_data/'\n",
        "    try:\n",
        "        # Load full datasets\n",
        "        Pmat_full = pd.read_csv(f'{base_path}Pmat_full.csv')\n",
        "        Ppor_full = pd.read_csv(f'{base_path}Ppor_full.csv')\n",
        "        \n",
        "        # Load gender splits\n",
        "        PmatM = pd.read_csv(f'{base_path}PmatM.csv')\n",
        "        PmatFE = pd.read_csv(f'{base_path}PmatFE.csv')\n",
        "        PporM = pd.read_csv(f'{base_path}PporM.csv')\n",
        "        PporFE = pd.read_csv(f'{base_path}PporFE.csv')\n",
        "        \n",
        "        return {\n",
        "            'Pmat_full': Pmat_full,\n",
        "            'Ppor_full': Ppor_full,\n",
        "            'PmatM': PmatM,\n",
        "            'PmatFE': PmatFE,\n",
        "            'PporM': PporM,\n",
        "            'PporFE': PporFE\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading datasets: {e}\")\n",
        "        return None\n",
        "\n",
        "# Load all datasets\n",
        "datasets = load_processed_datasets()\n",
        "print(\"\\nVerifying datasets after loading:\")\n",
        "for name, df in datasets.items():\n",
        "    print(f\"{name}: {df.shape[0]} rows, {df.columns.tolist()[:5]} columns\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Calculating new features from existing features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature calculation function\n",
        "def calculate_features(df):\n",
        "    \"\"\"Calculate engineered features\"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # Calculate grade average\n",
        "    df['Gvg'] = df[['G1', 'G2']].mean(axis=1)\n",
        "    \n",
        "    # Calculate alcohol average\n",
        "    df['Avgalc'] = df[['Dalc', 'Walc']].mean(axis=1)\n",
        "    \n",
        "    # Calculate risk factor\n",
        "    df['Bum'] = (2.0 * df['failures'] + \n",
        "                 1.5 * df['absences'] + \n",
        "                 1.0 * df['Dalc'] + \n",
        "                 1.0 * df['Walc'] + \n",
        "                 1.0 * (5 - df['studytime']) + \n",
        "                 1.0 * df['freetime'])\n",
        "    \n",
        "    # Drop any rows with NaN values\n",
        "    df = df.dropna()\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoding categorical and nomial attributes of our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode_categorical_data(df):\n",
        "    \"\"\"Encode categorical variables and clean data\"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # Clean string values by removing quotes\n",
        "    for col in df.select_dtypes(include=['object']).columns:\n",
        "        df[col] = df[col].str.strip(\"'\")\n",
        "    \n",
        "    # Drop completely empty columns\n",
        "    df = df.dropna(axis=1, how='all')\n",
        "    \n",
        "    # Encode categorical variables\n",
        "    categorical_cols = ['Mjob', 'Fjob', 'reason', 'guardian', 'Pstatus']\n",
        "    df = pd.get_dummies(df, columns=categorical_cols, prefix=categorical_cols)\n",
        "    \n",
        "    # Encode binary variables\n",
        "    binary_mapping = {\n",
        "        'schoolsup': {'no': 0, 'yes': 1},\n",
        "        'famsup': {'no': 0, 'yes': 1},\n",
        "        'paid': {'no': 0, 'yes': 1},\n",
        "        'activities': {'no': 0, 'yes': 1},\n",
        "        'nursery': {'no': 0, 'yes': 1},\n",
        "        'higher': {'no': 0, 'yes': 1},\n",
        "        'internet': {'no': 0, 'yes': 1},\n",
        "        'romantic': {'no': 0, 'yes': 1},\n",
        "        'school': {'GP': 0, 'MS': 1},\n",
        "        'sex': {'F': 0, 'M': 1},\n",
        "        'address': {'U': 0, 'R': 1},\n",
        "        'famsize': {'LE3': 0, 'GT3': 1}\n",
        "    }\n",
        "    \n",
        "    for col, mapping in binary_mapping.items():\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].map(mapping)\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Apply features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing Pmat_full dataset...\n",
            "Original shape: (394, 33)\n",
            "Enhanced shape: (394, 50)\n",
            "Features: ['Avgalc', 'Bum', 'Dalc', 'Fedu', 'Fjob_at_home', 'Fjob_health', 'Fjob_other', 'Fjob_services', 'Fjob_teacher', 'G1', 'G2', 'G3', 'Gvg', 'Medu', 'Mjob_at_home', 'Mjob_health', 'Mjob_other', 'Mjob_services', 'Mjob_teacher', 'Pstatus_A', 'Pstatus_T', 'Walc', 'absences', 'activities', 'address', 'age', 'failures', 'famrel', 'famsize', 'famsup', 'freetime', 'goout', 'guardian_father', 'guardian_mother', 'guardian_other', 'health', 'higher', 'internet', 'nursery', 'paid', 'reason_course', 'reason_home', 'reason_other', 'reason_reputation', 'romantic', 'school', 'schoolsup', 'sex', 'studytime', 'traveltime']\n",
            "\n",
            "Processing Ppor_full dataset...\n",
            "Original shape: (648, 33)\n",
            "Enhanced shape: (648, 50)\n",
            "Features: ['Avgalc', 'Bum', 'Dalc', 'Fedu', 'Fjob_at_home', 'Fjob_health', 'Fjob_other', 'Fjob_services', 'Fjob_teacher', 'G1', 'G2', 'G3', 'Gvg', 'Medu', 'Mjob_at_home', 'Mjob_health', 'Mjob_other', 'Mjob_services', 'Mjob_teacher', 'Pstatus_A', 'Pstatus_T', 'Walc', 'absences', 'activities', 'address', 'age', 'failures', 'famrel', 'famsize', 'famsup', 'freetime', 'goout', 'guardian_father', 'guardian_mother', 'guardian_other', 'health', 'higher', 'internet', 'nursery', 'paid', 'reason_course', 'reason_home', 'reason_other', 'reason_reputation', 'romantic', 'school', 'schoolsup', 'sex', 'studytime', 'traveltime']\n",
            "\n",
            "Processing PmatM dataset...\n",
            "Original shape: (186, 33)\n",
            "Enhanced shape: (186, 50)\n",
            "Features: ['Avgalc', 'Bum', 'Dalc', 'Fedu', 'Fjob_at_home', 'Fjob_health', 'Fjob_other', 'Fjob_services', 'Fjob_teacher', 'G1', 'G2', 'G3', 'Gvg', 'Medu', 'Mjob_at_home', 'Mjob_health', 'Mjob_other', 'Mjob_services', 'Mjob_teacher', 'Pstatus_A', 'Pstatus_T', 'Walc', 'absences', 'activities', 'address', 'age', 'failures', 'famrel', 'famsize', 'famsup', 'freetime', 'goout', 'guardian_father', 'guardian_mother', 'guardian_other', 'health', 'higher', 'internet', 'nursery', 'paid', 'reason_course', 'reason_home', 'reason_other', 'reason_reputation', 'romantic', 'school', 'schoolsup', 'sex', 'studytime', 'traveltime']\n",
            "\n",
            "Processing PmatFE dataset...\n",
            "Original shape: (208, 33)\n",
            "Enhanced shape: (208, 50)\n",
            "Features: ['Avgalc', 'Bum', 'Dalc', 'Fedu', 'Fjob_at_home', 'Fjob_health', 'Fjob_other', 'Fjob_services', 'Fjob_teacher', 'G1', 'G2', 'G3', 'Gvg', 'Medu', 'Mjob_at_home', 'Mjob_health', 'Mjob_other', 'Mjob_services', 'Mjob_teacher', 'Pstatus_A', 'Pstatus_T', 'Walc', 'absences', 'activities', 'address', 'age', 'failures', 'famrel', 'famsize', 'famsup', 'freetime', 'goout', 'guardian_father', 'guardian_mother', 'guardian_other', 'health', 'higher', 'internet', 'nursery', 'paid', 'reason_course', 'reason_home', 'reason_other', 'reason_reputation', 'romantic', 'school', 'schoolsup', 'sex', 'studytime', 'traveltime']\n",
            "\n",
            "Processing PporM dataset...\n",
            "Original shape: (265, 33)\n",
            "Enhanced shape: (265, 50)\n",
            "Features: ['Avgalc', 'Bum', 'Dalc', 'Fedu', 'Fjob_at_home', 'Fjob_health', 'Fjob_other', 'Fjob_services', 'Fjob_teacher', 'G1', 'G2', 'G3', 'Gvg', 'Medu', 'Mjob_at_home', 'Mjob_health', 'Mjob_other', 'Mjob_services', 'Mjob_teacher', 'Pstatus_A', 'Pstatus_T', 'Walc', 'absences', 'activities', 'address', 'age', 'failures', 'famrel', 'famsize', 'famsup', 'freetime', 'goout', 'guardian_father', 'guardian_mother', 'guardian_other', 'health', 'higher', 'internet', 'nursery', 'paid', 'reason_course', 'reason_home', 'reason_other', 'reason_reputation', 'romantic', 'school', 'schoolsup', 'sex', 'studytime', 'traveltime']\n",
            "\n",
            "Processing PporFE dataset...\n",
            "Original shape: (383, 33)\n",
            "Enhanced shape: (383, 50)\n",
            "Features: ['Avgalc', 'Bum', 'Dalc', 'Fedu', 'Fjob_at_home', 'Fjob_health', 'Fjob_other', 'Fjob_services', 'Fjob_teacher', 'G1', 'G2', 'G3', 'Gvg', 'Medu', 'Mjob_at_home', 'Mjob_health', 'Mjob_other', 'Mjob_services', 'Mjob_teacher', 'Pstatus_A', 'Pstatus_T', 'Walc', 'absences', 'activities', 'address', 'age', 'failures', 'famrel', 'famsize', 'famsup', 'freetime', 'goout', 'guardian_father', 'guardian_mother', 'guardian_other', 'health', 'higher', 'internet', 'nursery', 'paid', 'reason_course', 'reason_home', 'reason_other', 'reason_reputation', 'romantic', 'school', 'schoolsup', 'sex', 'studytime', 'traveltime']\n"
          ]
        }
      ],
      "source": [
        "def process_and_save_datasets(datasets):\n",
        "    \"\"\"Process datasets and save enhanced versions\"\"\"\n",
        "    base_path = 'processed_data/'\n",
        "    enhanced_datasets = {}\n",
        "    \n",
        "    for name, df in datasets.items():\n",
        "        print(f\"\\nProcessing {name} dataset...\")\n",
        "        try:\n",
        "            # First encode and clean data\n",
        "            encoded_df = encode_categorical_data(df)\n",
        "            \n",
        "            # Then add engineered features\n",
        "            enhanced_df = calculate_features(encoded_df)\n",
        "            \n",
        "            # Verify we have valid data\n",
        "            if enhanced_df.empty:\n",
        "                print(f\"Warning: {name} dataset is empty after processing\")\n",
        "                continue\n",
        "            \n",
        "            # Save enhanced dataset\n",
        "            output_path = f'{base_path}X_{name}_enhanced.csv'\n",
        "            enhanced_df.to_csv(output_path, index=False)\n",
        "            enhanced_datasets[name] = enhanced_df\n",
        "            \n",
        "            # Print processing results\n",
        "            print(f\"Original shape: {df.shape}\")\n",
        "            print(f\"Enhanced shape: {enhanced_df.shape}\")\n",
        "            print(f\"Features: {sorted(enhanced_df.columns.tolist())}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {name}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    return enhanced_datasets\n",
        "\n",
        "# Process datasets\n",
        "processed_datasets = process_and_save_datasets(datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyse features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Feature Engineering Statistics:\n",
            "\n",
            "Pmat_full Dataset Statistics:\n",
            "\n",
            "Gvg:\n",
            "  Mean: 10.82\n",
            "  Std: 3.41\n",
            "  Min: 2.00\n",
            "  Max: 19.00\n",
            "\n",
            "Avgalc:\n",
            "  Mean: 1.88\n",
            "  Std: 0.98\n",
            "  Min: 1.00\n",
            "  Max: 5.00\n",
            "\n",
            "Bum:\n",
            "  Mean: 19.13\n",
            "  Std: 12.75\n",
            "  Min: 5.00\n",
            "  Max: 118.50\n",
            "\n",
            "Ppor_full Dataset Statistics:\n",
            "\n",
            "Gvg:\n",
            "  Mean: 11.49\n",
            "  Std: 2.73\n",
            "  Min: 2.00\n",
            "  Max: 18.50\n",
            "\n",
            "Avgalc:\n",
            "  Mean: 1.89\n",
            "  Std: 0.99\n",
            "  Min: 1.00\n",
            "  Max: 5.00\n",
            "\n",
            "Bum:\n",
            "  Mean: 15.93\n",
            "  Std: 8.07\n",
            "  Min: 4.00\n",
            "  Max: 59.00\n",
            "\n",
            "PmatM Dataset Statistics:\n",
            "\n",
            "Gvg:\n",
            "  Mean: 11.17\n",
            "  Std: 3.50\n",
            "  Min: 2.50\n",
            "  Max: 19.00\n",
            "\n",
            "Avgalc:\n",
            "  Mean: 2.18\n",
            "  Std: 1.13\n",
            "  Min: 1.00\n",
            "  Max: 5.00\n",
            "\n",
            "Bum:\n",
            "  Mean: 19.42\n",
            "  Std: 10.16\n",
            "  Min: 5.00\n",
            "  Max: 68.00\n",
            "\n",
            "PmatFE Dataset Statistics:\n",
            "\n",
            "Gvg:\n",
            "  Mean: 10.50\n",
            "  Std: 3.30\n",
            "  Min: 2.00\n",
            "  Max: 18.50\n",
            "\n",
            "Avgalc:\n",
            "  Mean: 1.61\n",
            "  Std: 0.73\n",
            "  Min: 1.00\n",
            "  Max: 5.00\n",
            "\n",
            "Bum:\n",
            "  Mean: 18.87\n",
            "  Std: 14.70\n",
            "  Min: 6.00\n",
            "  Max: 118.50\n",
            "\n",
            "PporM Dataset Statistics:\n",
            "\n",
            "Gvg:\n",
            "  Mean: 11.15\n",
            "  Std: 2.63\n",
            "  Min: 2.00\n",
            "  Max: 18.00\n",
            "\n",
            "Avgalc:\n",
            "  Mean: 2.28\n",
            "  Std: 1.16\n",
            "  Min: 1.00\n",
            "  Max: 5.00\n",
            "\n",
            "Bum:\n",
            "  Mean: 17.35\n",
            "  Std: 8.41\n",
            "  Min: 6.00\n",
            "  Max: 53.00\n",
            "\n",
            "PporFE Dataset Statistics:\n",
            "\n",
            "Gvg:\n",
            "  Mean: 11.73\n",
            "  Std: 2.78\n",
            "  Min: 2.50\n",
            "  Max: 18.50\n",
            "\n",
            "Avgalc:\n",
            "  Mean: 1.61\n",
            "  Std: 0.74\n",
            "  Min: 1.00\n",
            "  Max: 5.00\n",
            "\n",
            "Bum:\n",
            "  Mean: 14.94\n",
            "  Std: 7.69\n",
            "  Min: 4.00\n",
            "  Max: 59.00\n"
          ]
        }
      ],
      "source": [
        "def analyze_features(processed_datasets):\n",
        "    print(\"\\nFeature Engineering Statistics:\")\n",
        "    \n",
        "    for name, df in processed_datasets.items():\n",
        "        if df is None or df.empty:\n",
        "            print(f\"\\n{name} Dataset is empty or None\")\n",
        "            continue\n",
        "            \n",
        "        print(f\"\\n{name} Dataset Statistics:\")\n",
        "        \n",
        "        new_features = ['Gvg', 'Avgalc', 'Bum']\n",
        "        for feature in new_features:\n",
        "            if feature in df.columns:\n",
        "                stats = df[feature].describe()\n",
        "                print(f\"\\n{feature}:\")\n",
        "                print(f\"  Mean: {stats['mean']:.2f}\")\n",
        "                print(f\"  Std: {stats['std']:.2f}\")\n",
        "                print(f\"  Min: {stats['min']:.2f}\")\n",
        "                print(f\"  Max: {stats['max']:.2f}\")\n",
        "            else:\n",
        "                print(f\"\\n{feature}: Not available for this dataset\")\n",
        "\n",
        "# Analyze all processed datasets\n",
        "if processed_datasets:\n",
        "    analyze_features(processed_datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Splitting and saving this data for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing Pmat_full dataset for train-test split...\n",
            "\n",
            "Pmat_full:\n",
            "Training: 275 samples\n",
            "Testing: 119 samples\n",
            "\n",
            "Processing Ppor_full dataset for train-test split...\n",
            "\n",
            "Ppor_full:\n",
            "Training: 453 samples\n",
            "Testing: 195 samples\n",
            "\n",
            "Processing PmatM dataset for train-test split...\n",
            "\n",
            "PmatM:\n",
            "Training: 130 samples\n",
            "Testing: 56 samples\n",
            "\n",
            "Processing PmatFE dataset for train-test split...\n",
            "\n",
            "PmatFE:\n",
            "Training: 145 samples\n",
            "Testing: 63 samples\n",
            "\n",
            "Processing PporM dataset for train-test split...\n",
            "\n",
            "PporM:\n",
            "Training: 185 samples\n",
            "Testing: 80 samples\n",
            "\n",
            "Processing PporFE dataset for train-test split...\n",
            "\n",
            "PporFE:\n",
            "Training: 268 samples\n",
            "Testing: 115 samples\n"
          ]
        }
      ],
      "source": [
        "def split_save_and_print(data, name, test_size=0.3, random_state=42):\n",
        "    \"\"\"Split dataset into training and testing sets\"\"\"\n",
        "    if data is None or len(data) == 0:\n",
        "        print(f\"\\nWarning: {name} dataset is empty, skipping split\")\n",
        "        return None, None, None, None\n",
        "    \n",
        "    # Remove sex column if present (already split by gender)\n",
        "    features_to_drop = ['G1', 'G2', 'G3']\n",
        "    if 'sex' in data.columns:\n",
        "        features_to_drop.append('sex')\n",
        "        \n",
        "    X = data.drop(features_to_drop, axis=1)\n",
        "    y = data[['G1', 'G2', 'G3']]\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y,\n",
        "        test_size=test_size,\n",
        "        random_state=random_state\n",
        "    )\n",
        "    \n",
        "    # Save splits\n",
        "    base_path = 'processed_data'\n",
        "    X_train.to_csv(f'{base_path}/X_{name}_train.csv', index=False)\n",
        "    X_test.to_csv(f'{base_path}/X_{name}_test.csv', index=False)\n",
        "    y_train.to_csv(f'{base_path}/y_{name}_train.csv', index=False)\n",
        "    y_test.to_csv(f'{base_path}/y_{name}_test.csv', index=False)\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"Training: {X_train.shape[0]} samples\")\n",
        "    print(f\"Testing: {X_test.shape[0]} samples\")\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Process and save datasets\n",
        "if processed_datasets:\n",
        "    for name, df in processed_datasets.items():\n",
        "        print(f\"\\nProcessing {name} dataset for train-test split...\")\n",
        "        split_save_and_print(df, name)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
