{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering\n",
        "\n",
        "### Engineered Features and Justifications\n",
        "\n",
        "1. **Gvg** - G1 and G2 average grade (integer of (G1 + G2) / 2)\n",
        "   - Justification: Averaging the first and second period grades provides a more stable measure of a student's performance over time, reducing the impact of any single period's anomalies.\n",
        "\n",
        "2. **Avgalc** - Average Dalc and Walc (integer of (Dalc + Walc) / 2)\n",
        "   - Justification: Combining workday and weekend alcohol consumption into a single average value gives a more comprehensive view of a student's overall alcohol consumption habits.\n",
        "\n",
        "3. **Bum** - A weighted sum of failures, absences, Dalc, Walc, inverted studytime, and freetime to indicate a student's tendency to fail, skip school, drink alcohol, not study, and have free time.\n",
        "   - Justification for weights:\n",
        "     - Failures are given a higher weight (2) because past class failures are a strong indicator of academic struggles.\n",
        "     - Absences are weighted at 1.5 as frequent absences can significantly impact academic performance.\n",
        "     - Both Dalc and Walc are weighted at 1 as alcohol consumption can affect both health and academic performance.\n",
        "     - Studytime is inverted (5 - studytime) and weighted at 1 because less study time can lead to poorer academic outcomes.\n",
        "     - Freetime is weighted at 1 as more free time might indicate less focus on academics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import dependancies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load the datasets\n",
        "Due to our intergration of pipelines, we do not need to convert from arff again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully loaded Pmat_full.csv: 251 rows\n",
            "Successfully loaded Ppor_full.csv: 461 rows\n",
            "Successfully loaded PmatM.csv: 119 rows\n",
            "Warning: File not found: processed_data/PmatF.csv\n",
            "Successfully loaded PporM.csv: 179 rows\n",
            "Warning: File not found: processed_data/PporF.csv\n"
          ]
        }
      ],
      "source": [
        "def load_dataset(filename):\n",
        "    \"\"\"Load a dataset with improved error handling\"\"\"\n",
        "    filepath = os.path.join('processed_data', filename)\n",
        "    \n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"Warning: File not found: {filepath}\")\n",
        "        return None\n",
        "    \n",
        "    try:\n",
        "        # Load CSV with proper handling of quoted strings\n",
        "        df = pd.read_csv(filepath, quotechar=\"'\", escapechar=\"\\\\\")\n",
        "        print(f\"Successfully loaded {filename}: {df.shape[0]} rows\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {filename}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Load all datasets\n",
        "datasets = {\n",
        "    'Pmat_full': load_dataset('Pmat_full.csv'),\n",
        "    'Ppor_full': load_dataset('Ppor_full.csv'),\n",
        "    'PmatM': load_dataset('PmatM.csv'),\n",
        "    'PmatF': load_dataset('PmatF.csv'),\n",
        "    'PporM': load_dataset('PporM.csv'),\n",
        "    'PporF': load_dataset('PporF.csv')\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoding categorical and nomial attributes of our data\n",
        "additionally, we will clean erronious data here before training. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Encoding categorical features...\n",
            "Encoding Pmat_full...\n",
            "Encoding Ppor_full...\n",
            "Encoding PmatM...\n",
            "Skipped PmatF (empty dataset)\n",
            "Encoding PporM...\n",
            "Skipped PporF (empty dataset)\n",
            "Categorical encoding complete\n"
          ]
        }
      ],
      "source": [
        "# Encode categorical features and clean data\n",
        "def encode_categorical_data(df):\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "        \n",
        "    df = df.copy()\n",
        "    \n",
        "    # Clean string values by removing quotes\n",
        "    for col in df.select_dtypes(include=['object']).columns:\n",
        "        df[col] = df[col].str.strip(\"'\")\n",
        "    \n",
        "    # Encode categorical variables\n",
        "    categorical_cols = ['school', 'sex', 'address', 'famsize', 'Mjob', 'Fjob', 'reason', 'guardian', 'Pstatus']\n",
        "    \n",
        "    # Handle binary variables first\n",
        "    binary_mapping = {\n",
        "        'schoolsup': {'no': 0, 'yes': 1},\n",
        "        'famsup': {'no': 0, 'yes': 1},\n",
        "        'paid': {'no': 0, 'yes': 1},\n",
        "        'activities': {'no': 0, 'yes': 1},\n",
        "        'nursery': {'no': 0, 'yes': 1},\n",
        "        'higher': {'no': 0, 'yes': 1},\n",
        "        'internet': {'no': 0, 'yes': 1},\n",
        "        'romantic': {'no': 0, 'yes': 1},\n",
        "        'school': {'GP': 0, 'MS': 1},\n",
        "        'sex': {'f': 0, 'm': 1, 'F': 0, 'M': 1},\n",
        "        'address': {'U': 0, 'R': 1},\n",
        "        'famsize': {'LE3': 0, 'GT3': 1},\n",
        "        'Pstatus': {'T': 0, 'A': 1}\n",
        "    }\n",
        "    \n",
        "    for col, mapping in binary_mapping.items():\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].map(mapping)\n",
        "    \n",
        "    # Remove binary columns from categorical columns\n",
        "    categorical_cols = [col for col in categorical_cols if col not in binary_mapping]\n",
        "    \n",
        "    # One-hot encode remaining categorical variables\n",
        "    if categorical_cols:\n",
        "        df = pd.get_dummies(df, columns=categorical_cols, prefix=categorical_cols)\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Process all datasets\n",
        "print(\"\\nEncoding categorical features...\")\n",
        "encoded_datasets = {}\n",
        "\n",
        "for name, df in datasets.items():\n",
        "    if df is not None and not df.empty:\n",
        "        print(f\"Encoding {name}...\")\n",
        "        encoded_datasets[name] = encode_categorical_data(df)\n",
        "    else:\n",
        "        encoded_datasets[name] = None\n",
        "        print(f\"Skipped {name} (empty dataset)\")\n",
        "\n",
        "# Replace original datasets with encoded versions\n",
        "datasets = encoded_datasets\n",
        "\n",
        "print(\"Categorical encoding complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create and apply features\n",
        "Refer to the top of his notebook for justifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing Pmat_full dataset...\n",
            "Processing Ppor_full dataset...\n",
            "Processing PmatM dataset...\n",
            "Warning: PmatF dataset is empty or None\n",
            "Processing PporM dataset...\n",
            "Warning: PporF dataset is empty or None\n"
          ]
        }
      ],
      "source": [
        "# Create engineered features\n",
        "def create_engineered_features(df):\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "        \n",
        "    df_eng = df.copy()\n",
        "    \n",
        "    # Create Gvg - average of G1 and G2\n",
        "    if 'G1' in df.columns and 'G2' in df.columns:\n",
        "        try:\n",
        "            df_eng['Gvg'] = ((pd.to_numeric(df['G1'], errors='coerce') + \n",
        "                             pd.to_numeric(df['G2'], errors='coerce')) / 2)\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating Gvg: {str(e)}\")\n",
        "    \n",
        "    # Create Avgalc - average alcohol consumption\n",
        "    if 'Dalc' in df.columns and 'Walc' in df.columns:\n",
        "        try:\n",
        "            df_eng['Avgalc'] = ((pd.to_numeric(df['Dalc'], errors='coerce') + \n",
        "                               pd.to_numeric(df['Walc'], errors='coerce')) / 2)\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating Avgalc: {str(e)}\")\n",
        "    \n",
        "    # Create Bum composite indicator\n",
        "    required_cols = ['failures', 'absences', 'Dalc', 'Walc', 'studytime', 'freetime']\n",
        "    if all(col in df.columns for col in required_cols):\n",
        "        try:\n",
        "            df_eng['Bum'] = (\n",
        "                2.0 * pd.to_numeric(df['failures'], errors='coerce') + \n",
        "                1.5 * pd.to_numeric(df['absences'], errors='coerce') + \n",
        "                1.0 * pd.to_numeric(df['Dalc'], errors='coerce') + \n",
        "                1.0 * pd.to_numeric(df['Walc'], errors='coerce') + \n",
        "                1.0 * (1.0 - pd.to_numeric(df['studytime'], errors='coerce')) + \n",
        "                0.5 * pd.to_numeric(df['freetime'], errors='coerce')\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating Bum: {str(e)}\")\n",
        "            \n",
        "    return df_eng\n",
        "\n",
        "# Process each dataset\n",
        "processed_datasets = {}\n",
        "for name, df in datasets.items():\n",
        "    if df is not None and not df.empty:\n",
        "        print(f\"Processing {name} dataset...\")\n",
        "        processed_datasets[name] = create_engineered_features(df)\n",
        "    else:\n",
        "        processed_datasets[name] = None\n",
        "        print(f\"Warning: {name} dataset is empty or None\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyse features\n",
        "Provides some feedback on if our application worked, while additionally providing metrics for MLOPs development cycle operations testing and improved model development with realtime statistic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Feature Engineering Statistics:\n",
            "\n",
            "Pmat_full Dataset Statistics:\n",
            "\n",
            "Gvg:\n",
            "  Mean: 11.74\n",
            "  Std: 3.04\n",
            "  Min: 5.00\n",
            "  Max: 18.50\n",
            "\n",
            "Avgalc:\n",
            "  Mean: 0.22\n",
            "  Std: 0.25\n",
            "  Min: 0.00\n",
            "  Max: 1.00\n",
            "\n",
            "Bum:\n",
            "  Mean: 1.56\n",
            "  Std: 0.80\n",
            "  Min: 0.25\n",
            "  Max: 3.85\n",
            "\n",
            "Ppor_full Dataset Statistics:\n",
            "\n",
            "Gvg:\n",
            "  Mean: 11.95\n",
            "  Std: 2.22\n",
            "  Min: 6.50\n",
            "  Max: 17.00\n",
            "\n",
            "Avgalc:\n",
            "  Mean: 0.22\n",
            "  Std: 0.25\n",
            "  Min: 0.00\n",
            "  Max: 1.00\n",
            "\n",
            "Bum:\n",
            "  Mean: 1.55\n",
            "  Std: 0.79\n",
            "  Min: 0.12\n",
            "  Max: 3.92\n",
            "\n",
            "PmatM Dataset Statistics:\n",
            "\n",
            "Gvg:\n",
            "  Mean: 12.47\n",
            "  Std: 2.90\n",
            "  Min: 5.50\n",
            "  Max: 18.50\n",
            "\n",
            "Avgalc:\n",
            "  Mean: 0.27\n",
            "  Std: 0.29\n",
            "  Min: 0.00\n",
            "  Max: 1.00\n",
            "\n",
            "Bum:\n",
            "  Mean: 1.78\n",
            "  Std: 0.83\n",
            "  Min: 0.33\n",
            "  Max: 3.85\n",
            "\n",
            "PmatF Dataset is empty or None\n",
            "\n",
            "PporM Dataset Statistics:\n",
            "\n",
            "Gvg:\n",
            "  Mean: 11.72\n",
            "  Std: 2.20\n",
            "  Min: 7.50\n",
            "  Max: 17.00\n",
            "\n",
            "Avgalc:\n",
            "  Mean: 0.30\n",
            "  Std: 0.29\n",
            "  Min: 0.00\n",
            "  Max: 1.00\n",
            "\n",
            "Bum:\n",
            "  Mean: 1.81\n",
            "  Std: 0.87\n",
            "  Min: 0.38\n",
            "  Max: 3.80\n",
            "\n",
            "PporF Dataset is empty or None\n"
          ]
        }
      ],
      "source": [
        "def analyze_features(processed_datasets):\n",
        "    print(\"\\nFeature Engineering Statistics:\")\n",
        "    \n",
        "    for name, df in processed_datasets.items():\n",
        "        if df is None or df.empty:\n",
        "            print(f\"\\n{name} Dataset is empty or None\")\n",
        "            continue\n",
        "            \n",
        "        print(f\"\\n{name} Dataset Statistics:\")\n",
        "        \n",
        "        new_features = ['Gvg', 'Avgalc', 'Bum']\n",
        "        for feature in new_features:\n",
        "            if feature in df.columns:\n",
        "                stats = df[feature].describe()\n",
        "                print(f\"\\n{feature}:\")\n",
        "                print(f\"  Mean: {stats['mean']:.2f}\")\n",
        "                print(f\"  Std: {stats['std']:.2f}\")\n",
        "                print(f\"  Min: {stats['min']:.2f}\")\n",
        "                print(f\"  Max: {stats['max']:.2f}\")\n",
        "            else:\n",
        "                print(f\"\\n{feature}: Not available for this dataset\")\n",
        "\n",
        "# Analyze all processed datasets\n",
        "if processed_datasets:\n",
        "    analyze_features(processed_datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scaling engineered features to fit our datasets\n",
        "Because we scaled our features in datawrngle were scaled, our additional engineered features need to be within the same scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Scaling Engineered Features\n",
            "Pmat_full: Scaled 3 features - Gvg, Avgalc, Bum\n",
            "Ppor_full: Scaled 3 features - Gvg, Avgalc, Bum\n",
            "PmatM: Scaled 3 features - Gvg, Avgalc, Bum\n",
            "PmatF: Dataset is empty or None, skipping scaling\n",
            "PporM: Scaled 3 features - Gvg, Avgalc, Bum\n",
            "PporF: Dataset is empty or None, skipping scaling\n",
            "Scaling Complete\n"
          ]
        }
      ],
      "source": [
        "engineered_features = ['Gvg', 'Avgalc', 'Bum']\n",
        "\n",
        "print(\"\\nScaling Engineered Features\")\n",
        "\n",
        "# Process each dataset\n",
        "for name, df in processed_datasets.items():\n",
        "    if df is not None and not df.empty:\n",
        "        # Find engineered features that exist and are non-binary numerical\n",
        "        features_to_scale = []\n",
        "        for col in engineered_features:\n",
        "            if col in df.columns:\n",
        "                try:\n",
        "                    # Check if it's numerical and non-binary\n",
        "                    unique_values = df[col].dropna().unique()\n",
        "                    if len(unique_values) > 2:  # Skip binary features\n",
        "                        features_to_scale.append(col)\n",
        "                except:\n",
        "                    continue\n",
        "        \n",
        "        # Apply scaling if we have features to scale\n",
        "        if features_to_scale:\n",
        "            scaler = MinMaxScaler()\n",
        "            # Scale each feature individually \n",
        "            for col in features_to_scale:\n",
        "                # Reshape for MinMaxScaler (needs 2D array)\n",
        "                values = df[col].values.reshape(-1, 1)\n",
        "                processed_datasets[name][col] = scaler.fit_transform(values).flatten()\n",
        "            \n",
        "            print(f\"{name}: Scaled {len(features_to_scale)} features - {', '.join(features_to_scale)}\")\n",
        "        else:\n",
        "            print(f\"{name}: No non-binary engineered features to scale\")\n",
        "    else:\n",
        "        print(f\"{name}: Dataset is empty or None, skipping scaling\")\n",
        "\n",
        "print(\"Scaling Complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Saving the full enhanced datasets\n",
        "To maintain our operational contingency and additonally implement enhanced datasets, with new or changed datawrngling or featuring, this solution allows for new file names while adding the _enhanced.csv suffix. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Saving Enhanced Datasets\n",
            "Saved: Pmat_full_enhanced.csv (251 rows, 49 columns)\n",
            "Saved: Ppor_full_enhanced.csv (461 rows, 49 columns)\n",
            "Saved: PmatM_enhanced.csv (119 rows, 49 columns)\n",
            "Skipped saving PmatF (empty dataset)\n",
            "Saved: PporM_enhanced.csv (179 rows, 49 columns)\n",
            "Skipped saving PporF (empty dataset)\n",
            "Enhanced datasets saved successfully\n"
          ]
        }
      ],
      "source": [
        "# Save enhanced datasets post-scaling\n",
        "print(\"\\nSaving Enhanced Datasets\")\n",
        "\n",
        "# Save all processed datasets with _enhanced suffix\n",
        "for name, df in processed_datasets.items():\n",
        "    if df is not None and not df.empty:\n",
        "        # Create enhanced filename\n",
        "        output_filename = f\"{name}_enhanced.csv\"\n",
        "        output_path = os.path.join('processed_data', output_filename)\n",
        "        \n",
        "        # Save to CSV\n",
        "        df.to_csv(output_path, index=False)\n",
        "        print(f\"Saved: {output_filename} ({df.shape[0]} rows, {df.shape[1]} columns)\")\n",
        "    else:\n",
        "        print(f\"Skipped saving {name} (empty dataset)\")\n",
        "\n",
        "print(\"Enhanced datasets saved successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Splitting and saving data for training and testing\n",
        "Additionally including our formatting of X and Y train and test sets intergrates with our chosen training and testing pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Creating Train/Test Splits for Enhanced Datasets\n",
            "Processing Pmat_full...\n",
            "  • Training: 175 samples\n",
            "  • Testing: 76 samples\n",
            "Processing Ppor_full...\n",
            "  • Training: 322 samples\n",
            "  • Testing: 139 samples\n",
            "Processing PmatM...\n",
            "  • Training: 83 samples\n",
            "  • Testing: 36 samples\n",
            "Skipped PmatF (empty dataset)\n",
            "Processing PporM...\n",
            "  • Training: 125 samples\n",
            "  • Testing: 54 samples\n",
            "Skipped PporF (empty dataset)\n",
            "All enhanced datasets split and saved\n"
          ]
        }
      ],
      "source": [
        "# Split and save enhanced datasets for training\n",
        "print(\"\\nCreating Train/Test Splits for Enhanced Datasets\")\n",
        "\n",
        "for name, df in processed_datasets.items():\n",
        "    if df is not None and not df.empty:\n",
        "        print(f\"Processing {name}...\")\n",
        "        \n",
        "        # Define features and target\n",
        "        features_to_drop = ['G1', 'G2', 'G3']\n",
        "        X = df.drop(features_to_drop, axis=1)\n",
        "        y = df[['G1', 'G2', 'G3']]\n",
        "        \n",
        "        # Create train/test split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.3, random_state=42)\n",
        "        \n",
        "        # Save with enhanced naming convention\n",
        "        base_path = 'processed_data'\n",
        "        X_train.to_csv(f'{base_path}/X_{name}_enhanced_train.csv', index=False)\n",
        "        X_test.to_csv(f'{base_path}/X_{name}_enhanced_test.csv', index=False)\n",
        "        y_train.to_csv(f'{base_path}/y_{name}_enhanced_train.csv', index=False)\n",
        "        y_test.to_csv(f'{base_path}/y_{name}_enhanced_test.csv', index=False)\n",
        "        \n",
        "        print(f\"  • Training: {X_train.shape[0]} samples\")\n",
        "        print(f\"  • Testing: {X_test.shape[0]} samples\")\n",
        "    else:\n",
        "        print(f\"Skipped {name} (empty dataset)\")\n",
        "\n",
        "print(\"All enhanced datasets split and saved\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
